{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"teaser\" style=' background-position: right center; background-size: 00px; background-repeat: no-repeat; \n",
    "    padding-top: 20px;\n",
    "    padding-right: 10px;\n",
    "    padding-bottom: 155px;\n",
    "    padding-left: 10px;\n",
    "    border-bottom: 4px solid #379;                    \n",
    "    border-top: 4px solid #379;\n",
    "    border-right: 4px solid #379;\n",
    "    border-left: 4px solid #379;' > \n",
    "\n",
    "   <div style=\"text-align:center\">\n",
    "    <b><font size=\"6.5\">Unsupervised learning applied to molecular data</font></b><br><br> \n",
    "    <b><font size=\"5.9\">Winter school RCTF 2021</font></b>       \n",
    "  </div>\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    " <font size=\"3.0\"> Author: Max Pinheiro Jr<sup>1</sup></font><br><br>\n",
    " <font size=\"3.0\"> <sup>1</sup>Aix Marseille University, CNRS, Marseille, France<br>\n",
    "<span class=\"rctf--last-updated\" data-version=\"v1.0.0\">[Last updated: Janeiro 16, 2021]</span>\n",
    "</p>   \n",
    "  \n",
    "<div> \n",
    "<img  style=\"float: left;\" src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/d/d4/Aix-Marseille_Universit%C3%A9_%28Logo%29.svg/1200px-Aix-Marseille_Universit%C3%A9_%28Logo%29.svg.png\" width=\"310\"> \n",
    "<img  style=\"float: right;\" src=\"https://rctf2018.sciencesconf.org/data/pages/logo_rfct_small_4.png\" width=\"120\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore some of the most relevant and commonly used unsupervised machine learning approaches for data analysis by taking ilustrative examples of molecular datasets to facilitate your understanding of the theory underpinning the methods. Instead of going deep into mathematical details we will dive into a few introductory examples of the methods using simplified or *toy* data to explain the methods and illustrate how they work in practice. Based on these examples, some hands-on exercises are proposed using real molecular data to consolidate your knowledge on the different topics of unsupervised learning presented in the lecture. The tutorial was fully designed to run in a Python environment, so a basic knowledge of this programming language is required. For those that are not very familiar with Python, I suggest you to take a time to check the [Python documentation](https://docs.python.org/3/) and the [Numpy manual](https://numpy.org/doc/1.18/) for clues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links to access the tutorial:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/maxjr82/RCTF2021/blob/main/tutorial/intro_unsupervised_learning.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/maxjr82/RCTF2021/blob/main/tutorial/intro_unsupervised_learning.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "\n",
    "- To be able to run all code cells in the *google colab* platform, it will be necessary to install the py3Dmol package. This can be done by copying the linux command shown below and running it in a normal code cell (note that to run any Linux command the code line must start with \"!\") of the google colab environment:<br><br> **! pip install py3Dmol**<br><br>\n",
    "\n",
    "- Before start, we should also download the molecular dataset QM7 just to make sure that this dataset will be available for the tutorial tasks. To do that, one should run the following Linux command in the code cell of google colab:<br><br> **! wget http://quantum-machine.org/data/qm7.mat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:41:59.912000Z",
     "start_time": "2021-01-16T09:41:59.909266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this code cell to install the packages you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "The topics covered by this tutorial are organized as follows:\n",
    " \n",
    "1. [Dimensionality reduction](#dim_reduction)\n",
    "    1. [Principal Component Analysis - PCA](#pca)\n",
    "       1. [Exercise 1](#ex1)\n",
    "    2. [How to work with nonlinear data? (K-PCA)](#kpca)\n",
    "       1. [Exercise 2](#ex2)\n",
    "2. [Manifold learning](#manifold_learning)    \n",
    "    1. [t-distributed stochastic neighbor embedding (t-SNE)](#tsne)\n",
    "3. [Clustering methods](#clustering)\n",
    "    1. [K-Means](#kmeans)\n",
    "       1. [Choosing the optimal number of clusters](#optimal_k)\n",
    "       2. [Exercise 3](#ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dim_reduction'></a>\n",
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "## Principal Component Analysis - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)** is a fast and flexible *unsupervised learning* approach commonly used for dimensionality reduction in data. The main idea behind this method is to find a new set of dimensions or **mutually orthonormal axes** (therefore, linearly independent) which are ranked according to the variance of data along them. These set of linearly uncorrelated axes are called *principal components* and can equivalently be defined as the directions that **maximizes the variance of the projected data** (that is, accounts for as much of the variability in the data as possible). More generally, the first (last) $k$ principal components ($k$ = 1, 2, 3...) explain the most (least) variance any $k$ variables can explain, under some general restrictions. Thus, PCA is a statistical analysis tool that aims to explain the maximum amount of variance in the data with the fewest number of principal components. Mathematically, the principal components are obtained by solving an eigenvalues problem where the PCs correspond to the eigenvectors of the data's covariance matrix given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cov(\\mathbf{X}) = \\frac{1}{n-1} \\left( (\\mathbf{X} - \\mathbf{\\bar{x}})^T\\;(\\mathbf{X} - \\mathbf{\\bar{x}}) \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{\\bar{x}}$ is the mean vector $\\mathbf{\\bar{x}} = \\frac{1}{n} \\sum\\limits_{i=1}^n x_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to remark that, as an unsupervised learning method, PCA does not require any labeled data. However, once the projection matrix derived from the $n$ first principal components has been determined, it is possible to define one metric to evaluate how good is the PCA decomposition for representing the data in a reduced subspace. This can be done by computing the so-called **reconstruction error** which basically measure how much the data projected onto the lower dimensional space spanned by the $n$ principal components deviates or differ from the original data as given by the Euclidean distance between the original and reconstructed data points. Since we are losing some information when truncating the PCA decomposition at a certain number of components, it would be desired to guarantee that the information contained in the discarded components is as small as possible. Thus, PCA can be also formulated as a minimization problem where the task is to find a new (orthonormal) basis set that [**minimizes the reconstruction error**](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/dimensionality/paper-1x2.pdf) of the data. It can be shown that the reconstruction error corresponds to the sum of the eigenvalues of the covariance matrix for the $m$ discarded principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA general applications:**\n",
    "\n",
    "  - visualization of high dimensional data \n",
    "  - reduce computational cost for ML\n",
    "  - feature extraction \n",
    "  - noise-filtration \n",
    "  \n",
    "**PCA in chemistry:**\n",
    "\n",
    "- [characterization of chemical pathway data from molecular dynamics simulations](https://pubs.rsc.org/en/content/articlelanding/2019/SC/C9SC02742D#!divAbstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more information about PCA:**\n",
    "\n",
    "- [Principal Component Analysis (PCA) clearly explained](https://www.youtube.com/watch?v=_UVHneBUBW0&ab_channel=StatQuestwithJoshStarmer)\n",
    "- [Lecture by Prof. Frank No√©](https://www.youtube.com/watch?v=z0H8mJtcgLA&ab_channel=FrankNoe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before start coding we will need to import some useful python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:08.591262Z",
     "start_time": "2021-01-16T09:41:59.996118Z"
    }
   },
   "outputs": [],
   "source": [
    "# Math libraries\n",
    "import numpy as np\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:08.603499Z",
     "start_time": "2021-01-16T09:42:08.595152Z"
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    '''\n",
    "    Implementation of the PCA method.\n",
    "    \n",
    "    Attributes of the class:\n",
    "        n_components: the number of components in which the data will be compressed.\n",
    "        \n",
    "    Argument of the functions:\n",
    "        X: A MxN dataset as NumPy array where the samples are stored as rows (M),\n",
    "           and the features defined as columns (N).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # STEP 1: Center the data by the mean, i.e, make the\n",
    "        # the sample mean of each column equal to zero.\n",
    "        # WARNING: check for outliers!\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X - self.mean\n",
    "        # STEP2: Calculate the covariance matrix of (X - X_mean)\n",
    "        # In numpy, the cov function needs samples as columns\n",
    "        cov = np.cov(X.T)\n",
    "        # STEP 3: Compute eigenvalues and eigenvectors\n",
    "        # np.eigh function returns them in ascending order\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        # STEP 4: sort eigenvectors and transpose them for easier calculations\n",
    "        eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n",
    "        eigvecs = eigvecs.T\n",
    "        # STEP 5: store the first n eigenvectors\n",
    "        self.components = eigvecs[0:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X - self.mean\n",
    "        # project the data along the n PCA components by computing the scalar product\n",
    "        X_trans = np.dot(X, self.components.T)\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first example, let's create a 2D array of data points representing a linear distribution with addition of a random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.508230Z",
     "start_time": "2021-01-16T09:42:08.607751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# First, create a grid of points within a given range\n",
    "t = np.linspace(-4,4,200)\n",
    "# Then, add some random noise to the first variable, t\n",
    "t = t + np.random.uniform(-0.2, 0.2, t.size)\n",
    "# Let's now create a gaussian random distribution\n",
    "gauss_noise = np.random.normal(0, 1.2, t.size)\n",
    "# Generate data from a linear equation: y = a*x + b\n",
    "a = b = 1\n",
    "y = (a*t + gauss_noise) + b\n",
    "# Concatenate the two variables to build the 2D dataset\n",
    "X = np.column_stack((t,y))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply our PCA algorithm keeping the 2 components of the feature space. Note that in this case we are not reducing the dimensionality of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.578617Z",
     "start_time": "2021-01-16T09:42:09.511769Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_transformed = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.653730Z",
     "start_time": "2021-01-16T09:42:09.584156Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the two PC's obtained after the transformation \n",
    "# These are eigenvectors of the covariance matrix\n",
    "print(\"*********************\")\n",
    "print(\"Principal components:\")\n",
    "print(\"*********************\")\n",
    "print(\" \")\n",
    "print(pca.components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can check that the PCA components are orthogonal by calculating the scalar product between them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.729718Z",
     "start_time": "2021-01-16T09:42:09.656516Z"
    }
   },
   "outputs": [],
   "source": [
    "scalar_product = np.dot(pca.components[:,0],pca.components[:,1].T)\n",
    "print(\"Scalar product of PC1 and PC2 = {}\".format(scalar_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance will essentially tell us what is the contribution to the percent variance of the entire PCA model contributed by each of the principal components (PC). In other words, how much PC1, PC2,..., PCn contribute to the overall variance of the dataset. Generally, the variance obtained for each principal component is presented in a decreasing orther, so PC1 should be the component contributing the most to the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.806705Z",
     "start_time": "2021-01-16T09:42:09.732927Z"
    }
   },
   "outputs": [],
   "source": [
    "def explained_variance(X_data):\n",
    "    '''This function calculates the fraction of variance explained by a principal component for an \n",
    "    input dataset X_data provided as a numpy array. To do that, the variance is calculated for each \n",
    "    column of the data and then divided by the total variance.''' \n",
    "    \n",
    "    n_sample, n_cols = X_data.shape\n",
    "    # compute the variance of the whole data set\n",
    "    total_var = (X_data**2).sum()/(n_sample-1)\n",
    "    # variance of selected columns\n",
    "    vars_col = np.array([np.var(X_data[:,i], ddof=1) for i in range(n_cols)])\n",
    "    explained_var_ratio = vars_col/total_var\n",
    "    \n",
    "    return explained_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:09.887013Z",
     "start_time": "2021-01-16T09:42:09.812334Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explained_var_original = explained_variance(X)\n",
    "explained_var_transformed = explained_variance(X_transformed)\n",
    "print(\"*************************\")\n",
    "print(\"Explained variance ratio:\")\n",
    "print(\"*************************\")\n",
    "print(\" \")\n",
    "print(\"Original data\")\n",
    "print(\"  X1  {:2.2f} \\n  X2  {:2.2f}\".format(explained_var_original[0]*100,explained_var_original[1]*100))\n",
    "print(\" \")\n",
    "print(\"Transformed data\")\n",
    "print(\"  PC1  {:2.2f} \\n  PC2  {:2.2f}\".format(explained_var_transformed[0]*100,\n",
    "                                               explained_var_transformed[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after the PCA transformation most of the variance in the data is captured by the principal component 1 (94.6%). Furthermore, one can also see that the explained variance sum up to 1 (or 100%) in both datasets (original and transformed). This is due to the fact that PCA is a deterministic approach, and therefore all the variance in the original data is still fully contained in the principal components once we do not truncate them to a dimension lower than that of the original data space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA analysis as well as other dimensionality reduction techniques have been also implemented in external packages that can be used in the Python environment. **Scikit-learn** is one of the most popular machine learning toolkits for Python which offers a quite efficient implementation of PCA. Since we are going to work later with much bigger datasets. let's use Scikit-learn the package for the next steps of the tutorial and compare the results with those obtained in our simplified class model. <br><br> For more details about the PCA library of scikit-learn, check the documentation in the link: <br> https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:11.295719Z",
     "start_time": "2021-01-16T09:42:09.890649Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_n2 = PCA(n_components=2)\n",
    "pca_n2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:11.303709Z",
     "start_time": "2021-01-16T09:42:11.299084Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trans_pc2 = pca_n2.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the PCA components and the explained variance ratio calculated using the scikit-learn library with those ones obtained previously with our PCA implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:11.399032Z",
     "start_time": "2021-01-16T09:42:11.306753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"**********************************\")\n",
    "print(\"Principal components from sklearn:\")\n",
    "print(\"**********************************\")\n",
    "print(\" \")\n",
    "print(pca_n2.components_)\n",
    "print(\" \")\n",
    "print(\"**************************************\")\n",
    "print(\"Explained variance ratio from sklearn:\")\n",
    "print(\"**************************************\")\n",
    "print(\" \")\n",
    "print(pca_n2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purpose, let's also project the data with PCA using only one principal component. In this case, the reduced data can be plotted along a single straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:11.474670Z",
     "start_time": "2021-01-16T09:42:11.400991Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_n1 = PCA(n_components=1)\n",
    "pca_n1.fit(X)\n",
    "X_trans_pc1 = pca_n1.transform(X)\n",
    "X_reconstructed = pca_n1.inverse_transform(X_trans_pc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:11.586288Z",
     "start_time": "2021-01-16T09:42:11.477465Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.zeros((X_trans_pc1.shape[0]))\n",
    "a = a.reshape(-1,1)\n",
    "X_trans_pc1 = np.append(X_trans_pc1, a, axis = 1)\n",
    "X_trans_pc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:12.440213Z",
     "start_time": "2021-01-16T09:42:11.589224Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "fig, (ax1, ax2) =  plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "\n",
    "# plot the original data on ax1\n",
    "ax1.scatter(X[:, 0], X[:, 1], alpha=0.3, c=\"blue\", label = \"original\")\n",
    "ax1.scatter(X_reconstructed[:,0], X_reconstructed[:,1], alpha=0.5, \n",
    "            marker = \"D\", c=\"red\", label = \"reduced\")\n",
    "for length, vector in zip(pca_n2.explained_variance_, pca_n2.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca_n2.mean_, pca_n2.mean_ + v, ax1)\n",
    "ax1.set_xlabel(\"X1\")\n",
    "ax1.set_ylabel(\"X2\")\n",
    "ax1.set_title(\"Original data vs PC1\", fontsize=16)\n",
    "ax1.axis('equal')\n",
    "ax1.legend(loc=\"upper left\", shadow=True, fancybox=True, fontsize=14)\n",
    "\n",
    "# plot the projected data on ax2\n",
    "ax2.scatter(X_trans_pc2[:,0], X_trans_pc2[:,1], alpha=0.3,\n",
    "            c=\"blue\", label = \"n_components = 2\")\n",
    "ax2.scatter(X_trans_pc1[:,0], X_trans_pc1[:,1], alpha=0.5,\n",
    "            marker = \"D\", c=\"red\", label = \"n_components = 1\")\n",
    "ax2.set_xlabel(\"Principal Component 1\")\n",
    "ax2.set_ylabel(\"Principal Component 2\")\n",
    "ax2.set_title(\"Transformed data (PC1 vs PC2)\", fontsize=16)\n",
    "ax2.axis('equal')\n",
    "ax2.legend(loc=\"upper right\", shadow=True, fancybox=True, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visually inspecting the two plots shown above, one can clearly see that the largest variance of the data is distributed along the *first* principal component (PC1). This is also evidenced by the arrows drawn on the left plot which represents the *principal axes* of the data with the relative importance of each ax for describing the distribution of the data indicated by the length of the corresponding vector. Moreover, it should be noted that if all data points are projected along PC1 most of the information related to the variance of the data will be still maintained (see red points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The QM7 dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the PCA technique with a more challenge dataset derived from quantum chemical calculations. The data selected for the analysis in the next exercises is the [QM7 dataset](http://quantum-machine.org/datasets/) which is composed of **7165 different organic molecules** having up to 23 atoms (including C, N, O, and S as possible combinations of atom types). Besides the 3D cartesian coordinates and atomic charges for each molecule, the QM7 dataset also includes the calculated **Coulomb Matrix** as molecular descriptor. So we will not need to build a molecular descriptor for now. The Coulomb matrix is a suitable vector representation for molecular systems based on the inverse pairwise distances between atoms (thereby, preserve some symmetry invariances). As target quantities, the QM7 also provides **atomization energies** (kcal/mol) calculated with DFT/PBE0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:12.825413Z",
     "start_time": "2021-01-16T09:42:12.442334Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "# These preprocessing functions of sklearn can be used for rescaling data\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.211766Z",
     "start_time": "2021-01-16T09:42:12.827415Z"
    }
   },
   "outputs": [],
   "source": [
    "# The qm7 dataset will be load as a python dictionary\n",
    "qm7 = scipy.io.loadmat('qm7.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.220998Z",
     "start_time": "2021-01-16T09:42:13.214085Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in qm7.keys():\n",
    "    if \"_\" not in k:\n",
    "        print(\"{} ---> {}\".format(k,qm7[k].shape))\n",
    "    else:\n",
    "        print(k, qm7[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R and Z matrices represent the Cartesian coordinates (in Bohr) and the atomic charge of each atom in the molecules. Can you guess which molecule corresponds to the index zero of this dataset? Let's take a look at the Z and R values for this molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.322196Z",
     "start_time": "2021-01-16T09:42:13.223794Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Atomic charges:\")\n",
    "print(qm7['Z'][0])\n",
    "print(\" \")\n",
    "print(\"Cartesian coordinates:\")\n",
    "print(qm7['R'][0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based only in the chemical composition of the output above, one can guess that the corresponding molecule is probably methane. But to be sure it would be much better if we can visualize the entire molecular structure in our Python working environment, isn't it? In fact, we can do this by using the py3Dmol library and the function provided in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.554807Z",
     "start_time": "2021-01-16T09:42:13.325039Z"
    }
   },
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "def view_molecule(data, index, style):\n",
    "    \n",
    "    bohr2ang = 0.529177249\n",
    "    symbols = {1:'H', 6:'C', 7:'N', 8:'O', 16: 'S'}\n",
    "    \n",
    "    idx_nonzero = data['Z'][index].nonzero()\n",
    "    Z = data['Z'][index][idx_nonzero]\n",
    "    n_atoms = Z.size\n",
    "    labels = np.vectorize(symbols.get)(Z)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    \n",
    "    coords = data['R'][index][0:n_atoms,:].reshape(-1,3) * bohr2ang\n",
    "       \n",
    "    xyz = np.concatenate((labels, coords), axis=1)\n",
    "    n_atoms = xyz.shape[0]\n",
    "    xyz_str = [str(i).strip('[]') for i in xyz]\n",
    "    geom = str(n_atoms) + '\\n' + ' ' + '\\n'\n",
    "    geom += '\\n'.join(xyz_str)\n",
    "    geom = geom.replace(\"'\", \"\")\n",
    "    \n",
    "    for k in style.keys():\n",
    "        assert k in ('line', 'stick', 'sphere', 'carton')\n",
    "    \n",
    "    molview = py3Dmol.view(width=350,height=350)\n",
    "    molview.addModel(geom,'xyz')\n",
    "        \n",
    "    molview.setStyle(style)\n",
    "    molview.setBackgroundColor('0xeeeeee')\n",
    "    molview.zoomTo()\n",
    "    \n",
    "    return molview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize any molecule of our dataset by providing as input to the *view_molecule* function, the full QM7 dataset in its original format (python dictionary), and the index of the molecule we would like to see. In addition, we can also define some parameter settings related to the display style for molecular visualization. Here is an example of how to use the visualization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.599402Z",
     "start_time": "2021-01-16T09:42:13.558223Z"
    }
   },
   "outputs": [],
   "source": [
    "s = {'stick': {'radius': .15}, 'sphere': {'scale': 0.20}}\n",
    "mol = view_molecule(qm7, 0, s)\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key 'X' in the qm7 dictionary is used to store the Coulomb matrix (CM) calculated for each molecule in the dataset. By definition, the dimensions of CM for a given molecule depends on the number of atoms as follows: N$_{atoms}$ x N$_{atoms}$. Thus, one can see that in the QM7 dataset all CM matrices are *padded with zeros* when the number of atoms in the molecule is smaller than 23 (maximum number of atoms in the dataset) in order to keep constant the dimension of the entire dataset. Consequently, the final data used for the ML modelling, which is essentially the flattened CM with 23 * 23 = 529 features, is expected to have a significant sparsity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.725458Z",
     "start_time": "2021-01-16T09:42:13.601658Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples, cm_rows, cm_cols = qm7['X'].shape\n",
    "# Transform the original tensor shape of the CM descriptor, qm7['X'], into a 2D matrix\n",
    "qm7_cm = qm7['X'].reshape(-1, cm_rows * cm_cols)\n",
    "qm7_cm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform our 2D data matrix, qm7_cm, into a [pandas](https://pandas.pydata.org/) data frame object to have a better visualization of the whole data in a nice table format. Remember that after reshaping the original data tensor as described in the cell above, the rows of the data frame contains one raw Coulomb matrix for each molecule. So the total number of rows corresponds to the number of molecules in the original dataset, while the number of columns is given by the total dimension of the Coulomb matrix, 23 x 23.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:13.803193Z",
     "start_time": "2021-01-16T09:42:13.727896Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_qm7_cm = pd.DataFrame(qm7_cm)\n",
    "df_qm7_cm.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important advantage of using pandas to represent our data in python is that this library has many different facilities to manipulate, organize and preprocess the data in a very efficient way. Moreover, it also has some built-in functions that can easily provide statistical summary of the data presented in a readable format, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:15.740027Z",
     "start_time": "2021-01-16T09:42:13.810281Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_qm7_cm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the statistical summary above, the feature columns (from 0 to 528) have quite different distributions with a large variation for both the mean values and standard deviation. Do you think that this might be a problem for PCA? Let's see more about this question in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:15.746458Z",
     "start_time": "2021-01-16T09:42:15.743803Z"
    }
   },
   "outputs": [],
   "source": [
    "# PCA is quite sensitive to the data scale (and outliers)\n",
    "# qm7_cm = MinMaxScaler().fit_transform(qm7_cm)\n",
    "# qm7_cm = StandardScaler().fit_transform(qm7_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.393323Z",
     "start_time": "2021-01-16T09:42:15.748662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the PCA object from sklearn library with the \n",
    "# desired number of components (n = 1, 2, 3,...)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(qm7_cm)\n",
    "\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.435553Z",
     "start_time": "2021-01-16T09:42:16.400325Z"
    }
   },
   "outputs": [],
   "source": [
    "qm7_cm_reduced = pca.transform(qm7_cm)\n",
    "qm7_cm_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.485175Z",
     "start_time": "2021-01-16T09:42:16.440209Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca_2d(X_reduced, color_by=None, cb_label=\"Property\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,7))\n",
    "    \n",
    "    if color_by is not None:\n",
    "        points = ax.scatter(X_reduced[:,0], X_reduced[:,1], alpha = 0.5, s=10, \n",
    "                            c=color_by, cmap=\"jet\")\n",
    "        cb = fig.colorbar(points, ax=ax, pad=0.02)\n",
    "        cb.set_label(cb_label, fontsize=16, labelpad=10)\n",
    "    else:\n",
    "        points = ax.scatter(X_reduced[:,0], X_reduced[:,1], alpha = 0.5, \n",
    "                            s=10, c=\"firebrick\")\n",
    "        \n",
    "    ax.set_xlabel(\"Principal component 1\", fontsize = 16)\n",
    "    ax.set_ylabel(\"Principal component 2\", fontsize = 16)\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    #ax.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex1'></a>\n",
    "## Exercise 1: PCA analysis for the QM7 dataset\n",
    "\n",
    "#### A. Use the plot function and the qm7_cm data provided in the cells above to make a scatter plot of the two principal components (PC1 and PC2) obtained for the PCA reduced data. Can you see any pattern in the distribution of data points? How much of the data's variance can be explained by these two components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:52.048167Z",
     "start_time": "2021-01-04T14:11:51.941545Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Create a new scatter plot for the compressed QM7 dataset (qm7_cm) but now coloring the points according to a certain property accessible through the dataset (e.g, number of atoms or atomization energy). Is it possible now to visually distinguish between different types of molecules present in the dataset? <br><br> Hint: To determine the total number of atoms per molecule, you can sum up all non-zero elements of the atomic charge vectors Z (check the *count_nonzero* function of numpy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:52.136497Z",
     "start_time": "2021-01-04T14:11:52.051003Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Choose one of the data scaling functions, MinMax or Standard, available in sklearn and then perform the PCA dimensionality reduction again for the rescaled data. How the colored scatter plot looks like when applying PCA on the rescaled data? Calculate the explained variance ratios again and compare them with the results of item A. Which one of the models (A or C) would you choose? <br><br> Hint: Check the examples provided in one of the cells above to apply the sklearn scaler.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about the importance of data scaling for PCA and other ML algorithms, check the link: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:52.224267Z",
     "start_time": "2021-01-04T14:11:52.139534Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:52.314254Z",
     "start_time": "2021-01-04T14:11:52.226380Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. [Optional] Calculate the cumulative explained variance ratio as a function of the number of components for the rescaled dataset and make a plot of the results. Based on this analysis, try to determine how many principal components one needs to consider in order to explain at least 60% of the total variance in the rescaled QM7 dataset. <br><br> Hint: For this task you can apply the *cumsum* function of numpy on the \"explained_variance_ratio_\" vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:52.421322Z",
     "start_time": "2021-01-04T14:11:52.316248Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kpca'></a>\n",
    "## How to work with nonlinear data? <br><br> Kernelized PCA - KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of PCA is that it is not able to model (or learn patterns from) nonlinear data which is a situation rather frequently found in the real-world scenario. The idea, in this case, is to apply some kind of nonlinear transformation that maps (using an appropriate mathematical function $\\Phi: \\mathbb{R}^p\\to\\mathbb{R}^q$) the original data (dimension $p$) into a higher-dimensional feature space (dimension $q$) whereby the data can be linearly separable or easily split into clusters (see an example in the video below). At a first glance, this idea can look like a contradiction since we are interested in reducing the dimensionality of the data rather than increasing it. However, if the transformation is applied properly, it turns out that the data should lie on a lower-dimensional subspace of the high dimensional data space generated by the transformation. In other words, we increase the dimensionality in order to be able to decrease it to obtain a meaningful representation of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.658284Z",
     "start_time": "2021-01-16T09:42:16.495846Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/3liCbRZPrZA\" \\\n",
    "      frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; \\\n",
    "      picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, this transformation is performed via a *kernel function* which represents a nonlinear mapping of a sample $\\mathbf{X}$ into a new feature space calculated as a dot product in the form $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)$, where $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are different data points. This approach gives rise to the so-called **[kernel principal component analysis](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)** (or KPCA for short) that can be considered as an extension of the \"classic\" PCA method. Due to the scalar product, the kernel function can be also viewed as a *similarity measure* between pair of data points. In KPCA, the covariance matrix can be expressed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cov = \\frac{1}{m} \\sum_{i=1}^m \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_i).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, instead of solving the eigenvalue problem directly for the covariance matrix as in PCA, it can be shown that in the kernel PCA algorithm the eigenvector equation is solved for the (centered) kernel matrix $\\mathbf{K}$. Since we need to evaluate all pairwise dot product between data points, the computational cost of building the KPCA's kernel matrix scales with the number of samples in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some popular kernel functions:**\n",
    "\n",
    "- polynomial $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j +c_0)^d$\n",
    "- gaussian or radial basis functions (RBF) $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp{ \\left(\\frac{-||\\mathbf{x}_i - \\mathbf{x}_j||_2^2}{2 \\sigma^2} \\right)} = \\exp{ \\left(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||_2^2 \\right)}$, where $||\\, . ||_2$ denotes the $l_2$ vector norm\n",
    "- sigmoid $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh( \\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + c_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more details about kernel PCA, check out the links provided below:**\n",
    "\n",
    "- [Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models](https://arxiv.org/abs/1207.3538)\n",
    "- [Lecture by David R. Thompson](https://www.youtube.com/watch?v=HbDHohXPLnU&ab_channel=caltech)\n",
    "- [Kernel PCA tutorial](https://www.youtube.com/watch?v=qC2GeVWSivw&ab_channel=TsungTaiYeh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious to see what is the \"magic\" behind this kernel transformation, let's start by considering a simple example with another 2D toy data where we will apply the KPCA method to see how it works in practice. First, we create a function to generate our nonlinear 2D dataset which consists of concentric circles with some gaussian noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.706943Z",
     "start_time": "2021-01-16T09:42:16.662612Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_circle(n_points, radius, noise=None):\n",
    "    np.random.seed(42)\n",
    "    t = np.linspace(0, 2*np.pi, n_points)\n",
    "    if noise is not None:\n",
    "        g_noise = np.random.normal(0, noise, t.size)\n",
    "    else:\n",
    "        g_noise = np.zeros(t.size)\n",
    "    x1 = (radius + g_noise) * np.sin(t) \n",
    "    x2 = (radius + g_noise) * np.cos(t)\n",
    "    y_true = np.sqrt(x1**2 + x2**2)\n",
    "    data = np.column_stack((x1,x2,y_true))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:16.785744Z",
     "start_time": "2021-01-16T09:42:16.709896Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_outer = make_circle(250, 1.0, 0.05) # class A\n",
    "c_inner = make_circle(250, 0.333, 0.05) # class B\n",
    "\n",
    "print(\"Shape of the circles:\")\n",
    "print(\"Inner ---> {}\".format(c_inner.shape))\n",
    "print(\"Outer ---> {}\".format(c_outer.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the two generated noisy-circles into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:17.083483Z",
     "start_time": "2021-01-16T09:42:16.788430Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.vstack((c_outer,c_inner))\n",
    "df_circles = pd.DataFrame(data, columns=['x1','x2','r'])\n",
    "df_circles['class'] = np.where((df_circles['r'] > 0.55), 'A', 'B')\n",
    "df_circles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine the PCA components to see how it works in a nonlinear data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:17.097118Z",
     "start_time": "2021-01-16T09:42:17.086621Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_circles[['x1','x2']].values\n",
    "pca = PCA(n_components=2)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# Now we add the principal components to the original \n",
    "# dataset, so we can visualize both results together\n",
    "df_circles['PC1'] = X_transformed[:,0]\n",
    "df_circles['PC2'] = X_transformed[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:17.249891Z",
     "start_time": "2021-01-16T09:42:17.100108Z"
    }
   },
   "outputs": [],
   "source": [
    "df_circles.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot both the original and transformed data as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:18.182640Z",
     "start_time": "2021-01-16T09:42:17.251831Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,13))\n",
    "# Plot of the original data points\n",
    "plt.subplot(1, 2, 1, aspect='equal')\n",
    "plt.title(\"Original space\")\n",
    "p1 = sns.scatterplot(data=df_circles, x=\"x1\", y=\"x2\", hue=\"r\")\n",
    "p1.text(0.80, 0.85, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p1.text(0.25, 0.44, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "ax = plt.gca()\n",
    "ax.legend_ = None\n",
    "# Plot of the data points transformed by PCA\n",
    "plt.subplot(1, 2, 2, aspect='equal')\n",
    "plt.title(\"Projected by PCA\")\n",
    "p2 = sns.scatterplot(data=df_circles, x=\"PC1\", y=\"PC2\", hue=\"r\")\n",
    "p2.text(0.80, 0.85, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p2.text(0.25, 0.44, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "plt.legend(bbox_to_anchor=(1.15, 1.0),borderaxespad=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an intuition of what the *kernel* transformation implicitly does, let's consider again the noise circles dataset and the make a 3D plot of the new data points generated by the following nonlinear transformation: $[x_1, x_2] \\rightarrow [x_1, x_2, x_1^2 + x_2^2]$. Note that to create the third dimension we only need to use combinations of the original feature vectors $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:18.311684Z",
     "start_time": "2021-01-16T09:42:18.186260Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "xA = df_circles[df_circles['class'] == 'A']['x1'].values\n",
    "yA = df_circles[df_circles['class'] == 'A']['x2'].values\n",
    "zA = xA**2 + yA**2\n",
    "\n",
    "xB = df_circles[df_circles['class'] == 'B']['x1'].values\n",
    "yB = df_circles[df_circles['class'] == 'B']['x2'].values\n",
    "zB = xB**2 + yB**2\n",
    "\n",
    "ax.set_xlabel(r\"x$_1$\")\n",
    "ax.set_ylabel(r\"x$_2$\")\n",
    "ax.set_zlabel(r\"x$_1^2$ + x$_2^2$\")\n",
    "\n",
    "ax.scatter(xA, yA, zA, c=\"red\")\n",
    "ax.scatter(xB, yB, zB, c=\"blue\")\n",
    "\n",
    "(x, y) = np.meshgrid(np.arange(-1, 1.1, 0.5), np.arange(-1, 1.1, 0.5))\n",
    "z = np.full(x.shape, 0.5)\n",
    "\n",
    "ax.plot_surface(x, y, z, color=\"green\", alpha = 0.3)\n",
    "\n",
    "ax.azim, ax.dist, ax.elev = (-45, 10, 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply the KPCA method implemented in the scikit-learn package on our dataset. Among the options ok kernel available in this package, here we will use the radial basis function as an example. For more details about different kernel approximations used in scikit-learn see the link:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/kernel_approximation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:18.678185Z",
     "start_time": "2021-01-16T09:42:18.314682Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=2)\n",
    "X_transformed = kpca.fit_transform(X)\n",
    "\n",
    "df_circles['KPC1'] = X_transformed[:,0]\n",
    "df_circles['KPC2'] = X_transformed[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:19.529588Z",
     "start_time": "2021-01-16T09:42:18.688222Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\n",
    "\n",
    "# Plot of the original data points\n",
    "ax1.set_title(\"Original space\")\n",
    "p1 = sns.scatterplot(data=df_circles, x=\"x1\", y=\"x2\", hue=\"r\", ax = ax1)\n",
    "p1.text(0.80, 0.85, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p1.text(0.25, 0.50, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "ax1.legend_ = None\n",
    "\n",
    "# Plot of the data points transformed by KPCA\n",
    "ax2.set_title(\"Projected by KPCA\")\n",
    "p2 = sns.scatterplot(data=df_circles, x=\"KPC1\", y=\"KPC2\", hue=\"r\", ax = ax2)\n",
    "p2.text(-0.25, 0.50, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p2.text(0.40, 0.50, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "ax2.legend(bbox_to_anchor=(1.15, 1.0),borderaxespad=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the examples provided above, it is now time to explore the KPCA method in a more challenging dataset. To this end, we will first load again the QM7 dataset and then store the Coulomb matrix features in a pandas dataframe that can be used as input to apply the KPCA dimensionality reduction using the *scikit-learn* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:42:19.726602Z",
     "start_time": "2021-01-16T09:42:19.532859Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The qm7 dataset will be load as a python dictionary\n",
    "qm7 = scipy.io.loadmat('qm7.mat')\n",
    "\n",
    "n_samples, cm_rows, cm_cols = qm7['X'].shape\n",
    "# Transform the original tensor shape of the descriptor qm7['X'] into a 2D matrix\n",
    "qm7_cm = qm7['X'].reshape(-1, cm_rows * cm_cols)\n",
    "\n",
    "# Rescaling the data \n",
    "qm7_cm = StandardScaler().fit_transform(qm7_cm)\n",
    "\n",
    "df_qm7_cm = pd.DataFrame(qm7_cm)\n",
    "df_qm7_cm.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex2'></a>\n",
    "## Exercise 2: KPCA dimensionality reduction for the QM7 dataset\n",
    "\n",
    "#### A. For the qm7_cm data provided in the cell above, apply the KPCA method with a gaussian kernel function (\"rbf\") to reduce the dimensionality of the data to 2D. Consider first the default value of the gamma hyperparameter used in scikit-learn. Make a scatter plot of the KPCA reduced data with a color map for the points based on the atomization energy.<br><br> Compare this result with the previous one obtained in exercise 1. In a qualitative sense, which one of the methods, PCA or KPCA, provide a more meaningful picture of the QM7 data distribution in the 2D reduced space? Try some other kernel function available in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) (polynomial, cosine etc) and check how it affects the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about kernel selection, check this paper:<br> Hyperparameter selection in kernel principal component analysis - http://thescipub.com/abstract/10.3844/jcssp.2014.1139.1150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:56.484043Z",
     "start_time": "2021-01-04T14:11:56.428274Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Perform again the KPCA dimensionality reduction for the QM7 dataset using a gaussian kernel function (\"rbf\") but now varying the gamma hyperparameter in the range of 0.002 to 0.02. How much the different gamma values affect the data distribution in the 2D scatter plot? Qualitatively, for which value of the gamma hyperparameter in this range the resulting scatter plot shows the best separability of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:56.836986Z",
     "start_time": "2021-01-04T14:11:56.486794Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. [Optional] Select a sub-sample of the QM7 dataset based on the results of item A (KPCA with RBF kernel). To do that, perform again the KPCA-RBF transformation on the original data, and then apply a filter on the QM7 dataset (use the dataframe df_qm7_cm and the atomization energy, qm7['T']) by selecting the set of points enclosed by the region KPCA1 $\\geq$ 0 and KPCA2 $\\leq$ 0. For this sub-sample, apply the KPCA reduction with two components and the RBF kernel, and visualize the final results in a scatter plot colored by the corresponding atomization energies. How many cluster appears in this case? Is there a clear correlation between the clusters and the atomization energy (i.e, points belonging to different clusters appears with clearly distinct colors)?<br><br> Finally, select a few points (two to five) beloging to two different clusters and check the chemical composition (atomic number, qm7['Z']) for the molecules corresponding to these points. How different are the selected molecules?<br><br> Hint: Note that the KPCA transformation does not change the order of the elements in the dataset, so the indices of the 2D matrix generated upon dimensionality reduction are exactly the same as the original df_qm7_cm dataframe. Hence, you can first obtain the indices of the 2D matrix corresponding to the points you want to select, and then filter the df_qm7_cm dataframe accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:56.935560Z",
     "start_time": "2021-01-04T14:11:56.840438Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:57.056182Z",
     "start_time": "2021-01-04T14:11:56.938594Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:11:57.136239Z",
     "start_time": "2021-01-04T14:11:57.058902Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='manifold_learning'></a>\n",
    "# Manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tsne'></a>\n",
    "## t-distributed stochastic neighbor embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**t-SNE**](https://www.oreilly.com/content/an-illustrated-introduction-to-the-t-sne-algorithm/) is an unsupervised learning algorithm based on a probabilistc model mainly used for nonlinear dimensionality reduction. The goal of this algorithm is to find a low dimensional (typically 2D or 3D) manifold representation of the original high dimensional data by trying to preserve as much as possible the *local structure* of the data while also revealing some meaningful information about the global cluster distribution of the data points and the smooth nonlinear variations along the dimensions. To this end, t-SNE maps the similarity between points in the original space into *Gaussian joint probabilities* by assigning a high (low) probability to similar (dissimilar) points while the similarities in the low dimensional embedded space are represented by *Student's t-distributions*. Then, the [Kullback-Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is used as a cost function by the t-SNE algorithm to minimize the differences between these two probability distributions with respect to the locations of the points in the map using gradient descent.<br><br> \n",
    "\n",
    "Unlike PCA, t-SNE has *hyperparameters* these are user-specified options that determine the output of the model. The Gaussian distribution or circle around each point in the high dimensional space can be adjusted using a hyperparameter called *perplexity* which influences the variance of the distribution (circle size). Larger values of perplexity increase the number of points within the neighborhood. As mentioned in the original [t-SNE paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf), the typical values of perplexity are between 5 and 50. Although, t-SNE can be quite robust to changes in the perplexity for some datasets, in practice it is recommended to explore different perplexity values (even larger than 50) by analyzing the results in multiple plots. Furthermore, there are other parameters related to the optimization proccess that may significantly affect the final data distribution obtained with t-SNE. Finally, because of its stochastic nature (and non-convex form of the KL cost function), the t-SNE algorithm can yield different results with repeating runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, a group from Google Brain published great essay in Distill about \"How to Use t-SNE Effectively\". In the article, they provide an interactive tool to explore the effect of various hyperparameters of t-SNE on several datasets of complex topological and see the convergence of the algorithm on real-time:\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the tutorial we will use the t-SNE algorithm implemented in the scikit-learn package. So before starting to import the library, take a quick look at the documentation of t-SNE in the link below:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know more about the intution and mathematical details of the t-SNE method, check out the following links:\n",
    "\n",
    "- [t-SNE clearly explained](https://erdem.pl/2020/04/t-sne-clearly-explained) (article)\n",
    "- [StatQuest: t-SNE, Clearly Explained](https://www.youtube.com/watch?v=NEaUSP4YerM&ab_channel=StatQuestwithJoshStarmer) (video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application of t-SNE in chemistry**:\n",
    "\n",
    "- [JCTC paper: t-SNE Embedding Method with the\n",
    "Least Information Loss for Macromolecular Simulations](https://pubs.acs.org/doi/10.1021/acs.jctc.8b00652)\n",
    "- [JCIM paper: Drug Discovery Maps, a Machine Learning Model That Visualizes and Predicts Kinome‚ÄìInhibitor Interaction Landscapes](https://pubs.acs.org/doi/10.1021/acs.jcim.8b00640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start by applying the t-SNE model on the standardized Coulomb matrix representation of the QM7 dataset (*df_qm7_cm*) as used in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:40.025319Z",
     "start_time": "2021-01-16T09:42:19.728790Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Here we select only two dimensions for projecting the data, and\n",
    "# consider the default values for the hyperparameters.\n",
    "tsne = TSNE(n_components=2)\n",
    "X_reduced = tsne.fit_transform(df_qm7_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:40.055375Z",
     "start_time": "2021-01-16T09:43:40.027834Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_tsne_2d(data, color_by=None, cb_label=\"Property\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,7))\n",
    "    \n",
    "    if color_by is not None:\n",
    "        points = ax.scatter(data[:,0], data[:,1], alpha = 0.5, s=10, \n",
    "                            c=color_by, cmap=\"jet\")\n",
    "        cb = fig.colorbar(points, ax=ax, pad=0.02)\n",
    "        cb.set_label(cb_label, fontsize=16, labelpad=10)\n",
    "    else:\n",
    "        points = ax.scatter(X_reduced[:,0], X_reduced[:,1], alpha = 0.5, \n",
    "                            s=10, c=\"firebrick\")\n",
    "        \n",
    "    ax.set_xlabel(\"X1\", fontsize = 16)\n",
    "    ax.set_ylabel(\"X2\", fontsize = 16)\n",
    "    ax.set_title(\"Dimensionality reduction with t-SNE\", fontsize=18)\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function above to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:40.565156Z",
     "start_time": "2021-01-16T09:43:40.063650Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_tsne_2d(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot, the t-SNE algorithm has identified several well separated clusters (more than in KPCA) with different compactness and sizes. This crude picture, however, does not provide much information about possible correlation between the variables. So if you are may wondering what kind of underlying pattern the algorithm has found in the high dimensional data or which feature of the molecules is contributing most to create the different clusters, you got the point! In fact, this is the core idea of unsupervised learning methods, that is first we try to visually identify some clustering structure in the data, and then we can examine in the details whether or not there are relevant properties that distinguish one cluster of points from another. To have a better picture of our data, let's make the plot again but now coloring each points according to the number of atoms in the molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.120323Z",
     "start_time": "2021-01-16T09:43:40.567440Z"
    }
   },
   "outputs": [],
   "source": [
    "n_atoms = np.count_nonzero(qm7['Z'], axis=1)\n",
    "atomization_energy = qm7['T'].flatten()\n",
    "iae = atomization_energy / n_atoms # Intensive atomization energy\n",
    "plot_tsne_2d(X_reduced, n_atoms, \"Number of atoms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is very clear that the total number of atoms is probably the dominating feature the algorithm is considering to split the data into different clusters. The reason for that may be due to the high sparsity of the Coulomb matrix (CM) representation for a dataset with a broad range of molecular sizes (remember that the CM is a matrix of dimension N$_{atoms}$ x N$_{atoms}$). So this does not look a very exciting result, but if we look carefully at the plot there are still some interesting patterns that deserve a better investigation. For example, the two red clusters (bottom and top) should have roughly the same number of atoms but they appear very far apart in the plot, which may indicate other significant differences in the chemical structure of the molecules. Let's take a look at the molecules belonging to these red clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.141156Z",
     "start_time": "2021-01-16T09:43:41.124519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the results in a pandas dataframe for further analysis\n",
    "df_tsne = pd.DataFrame(X_reduced, columns=['X1', 'X2'])\n",
    "df_tsne.shape\n",
    "\n",
    "# t-SNE is a stochastic algorithm, so repeated runs may lead to different results\n",
    "# Thus, setup your selection criteria according to your results for the plot shown above\n",
    "selection = ((df_tsne['X1'] >= 30) & (df_tsne['X2'] <= -25))\n",
    "idx_red_bottom = df_tsne[selection].index.values\n",
    "\n",
    "cluster_red_bottom = {'Z': None, 'R': None}\n",
    "cluster_red_bottom['Z'] = qm7['Z'][idx_red_bottom]\n",
    "cluster_red_bottom['R'] = qm7['R'][idx_red_bottom]\n",
    "\n",
    "selection = (df_tsne['X2'] >= 75)\n",
    "idx_red_top = df_tsne[selection].index.values\n",
    "\n",
    "cluster_red_top = {'Z': None, 'R': None}\n",
    "cluster_red_top['Z'] = qm7['Z'][idx_red_top]\n",
    "cluster_red_top['R'] = qm7['R'][idx_red_top]\n",
    "\n",
    "n_mols_c1 = len(idx_red_bottom)\n",
    "n_mols_c2 = len(idx_red_top)\n",
    "\n",
    "print(\"Number of molecules in each (red) cluster:\\n\")\n",
    "print(\"bottom ----> {}\".format(n_mols_c1))\n",
    "print(\"   top ----> {}\".format(n_mols_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the indices selected for each cluster, one can visualize the molecular structures associated to the cluster for direct comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.333900Z",
     "start_time": "2021-01-16T09:43:41.143668Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from ipywidgets import HBox\n",
    "from ipywidgets import IntSlider,interactive\n",
    "\n",
    "def mols_from_c1(index):\n",
    "    s = {'stick': {'radius': .15}, 'sphere': {'scale': 0.20}}\n",
    "    viewer = view_molecule(cluster_red_bottom, index, style=s)\n",
    "    return viewer.show()\n",
    "\n",
    "def mols_from_c2(index):\n",
    "    s = {'stick': {'radius': .15}, 'sphere': {'scale': 0.20}}\n",
    "    viewer = view_molecule(cluster_red_top, index, style=s)\n",
    "    return viewer.show()\n",
    "\n",
    "mols_c1 = interactive(mols_from_c1, index=IntSlider(min=0,max=(n_mols_c1-1), step=1))\n",
    "mols_c2 = interactive(mols_from_c2, index=IntSlider(min=0,max=(n_mols_c2-1), step=1))\n",
    "\n",
    "HBox([mols_c1,mols_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-22T14:53:24.734180Z",
     "start_time": "2020-12-22T14:53:24.725906Z"
    }
   },
   "source": [
    "Great! We have learned how to apply a nonlinear dimensionality reduction technique and how to interpret the results derived from it. By visualizing the molecular structures of similar size that t-SNE placed at two different clusters in the embedded space, we can gain an intuition of how the dataset is distributed in the chemical space. Of course, to obtain a more conclusive picture about the data from the t-SNE results, it would necessary to perform some additional quantitative analysis that account for other chemical properties of the molecules. For example, one could create a histogram with respect to the number of heavy atoms or the number of sp3 carbons or even try to count the number of rings in the molecule for two different clusters having similar number of atoms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "# Clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Clustering**](https://en.wikipedia.org/wiki/Cluster_analysis) refers to a very broad set of unsupervised machine learning techiniques used to recognize subgroups, or clusters, of input points from a data set that are related to each other and different from other groups of points based on some **similarity criteria**. In this sense, the general task is to programatically group objects (or data points) sharing a high similarity score into the same cluster while dissimilar objects should be assigned to different clusters. The greater the similarity (or homogeneity) within a group and the greater the difference between groups, the better or more distinct the clustering. This notion of similarity is present in all clustering algorithms that differ from one another essentially in relation to the mathematical formulation of what constitutes a cluster and how to efficiently find them. Popular clustering approaches include:\n",
    "\n",
    "- **Centroid-based** $\\rightarrow$ organizes the data into non-hierarchical clusters (K-Means, K-Medians)\n",
    "- **Density-based** $\\rightarrow$ connects areas of high sample density into clusters (DBSCAN, OPTICS)\n",
    "- **Hierarchical** $\\rightarrow$ creates a tree of clusters (Agglomerative)\n",
    "- **Distribution-based** $\\rightarrow$ assumes data is composed of distributions, such as Gaussian distributions (Gaussian mixture models)\n",
    "\n",
    "Evaluating the performance of clustering models is a quite challenging task since in most of the cases there is no information about classes distribution in the data. Considering only its internal structure, a \"good\" clustering usually involves a certain trade-off between *compactness* (intra-cluster cohesion) and *separability*. In addition, one might also evaluate a clustering algorithm based on the utility of the clustering in its intended application.\n",
    "\n",
    "Unlike the dimensionality reduction techiniques discussed before, clustering analysis can be applied directly in the original *n*-dimensional feature space of the data to discover its structure, in this case, distinct clusters.\n",
    "\n",
    "For an extensive list of clustering models, see [A Comprehensive Survey of Clustering Algorithms](https://link.springer.com/article/10.1007/s40745-015-0040-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kmeans'></a>\n",
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is one of the oldest but still very used and most popular algorithm for cluster analysis. The main idea is to partition the data set into a predefined number of distinct (and non-overlapping) clusters, $\\mathbf{k}$, by assigning each sample $\\mathbf{x}_i$ of the data to the cluster with the nearest mean (cluster centers or cluster centroid). The algorithm start by selecting (randomly) *k* centroids $C_1,...,C_k$ corresponding to the number of clusters desired. Then each data point is assigned to the closest centroid (**expectation step**) by computing the respective Euclidean distance such that the colection of points assigned to the same centroid forms the initial cluster. In the next, the coordinates of the centroids are updated according to the mean value of all data points assigned to each centroid in the previous step (**maximization step**). These procceses of assignment and centroid's update are repeated iteratively until the centroids remain the\n",
    "same, or match the previous iteration‚Äôs assignment. Thus, the idea behind K-means is to minimize the *within-cluster variation* given by the [sum of the squared error (SSE)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) between each data point and its closest centroid, which is a measure of how much the samples within a cluster differ from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To know more about k-means, watch these lectures:**\n",
    "\n",
    "- [Lecture 13.2 of the Machine Learning course (coursera) by Andrew Ng](https://www.youtube.com/watch?v=hDmNF9JG3lo&ab_channel=ArtificialIntelligence-AllinOne)\n",
    "- [StatQuest: K-means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means algorithm is also part of the giant scikit-learn library, and it has been implemented in a very efficient way. So here, instead of rewritting the K-Means algorithm from scratch, we will run the first iteration of the optimization process step-by-step such that we can gain a better understanding of how the algorithm works in practice. Let's do it!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to build a simple 2D data set having two randomly generated clusters in order to be able to visualize the steps of the algorithm using scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.414268Z",
     "start_time": "2021-01-16T09:43:41.336765Z"
    }
   },
   "outputs": [],
   "source": [
    "center1 = (10, 15)\n",
    "center2 = (-5, -5)\n",
    "distance = 10\n",
    "n_points = 10\n",
    "\n",
    "x1 = np.random.uniform(center1[0], center1[0] + distance, size=(n_points,)).T\n",
    "y1 = np.random.normal(center1[1], distance, size=(n_points,)).T \n",
    "l1 = np.array(['blue'] * x1.size)\n",
    "cluster1 = np.column_stack((x1,y1))\n",
    "\n",
    "x2 = np.random.uniform(center2[0], center2[0] + distance, size=(n_points,)).T\n",
    "y2 = np.random.normal(center2[1], distance, size=(n_points,)).T\n",
    "l2 = np.array(['red'] * x2.size)\n",
    "cluster2 = np.column_stack((x2,y2))\n",
    "\n",
    "data = np.concatenate((cluster1,cluster2))\n",
    "labels = np.concatenate((l1,l2))\n",
    "df_clusters = pd.DataFrame(data, columns=['x','y'])\n",
    "df_clusters['labels'] = None\n",
    "df_clusters.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Assign initial centroids \"randomly\" for the two clusters in the data set. Note that in this simple example we already know how many clusters we have (K=2), but this information is generally not known a priori. We will back to this point later. \n",
    "\n",
    "In order to guarantee that the initialized centroids will be not placed too far apart from the data points, we will select randomly the x and y coordinates of each centroid constrained by the limits (max and min) of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.828156Z",
     "start_time": "2021-01-16T09:43:41.416229Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_max, y_max = df_clusters['x'].max(), df_clusters['y'].max()\n",
    "x_min, y_min = df_clusters['x'].min(), df_clusters['y'].min()\n",
    "\n",
    "centroid1 = np.array([np.random.uniform(x_min,x_max), np.random.uniform(y_min,y_max)])\n",
    "centroid2 = np.array([np.random.uniform(x_max,x_min), np.random.uniform(y_max,y_min)])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 1\", fontsize = 18)\n",
    "plt.scatter(df_clusters.x, df_clusters.y, c=\"k\")\n",
    "plt.scatter(centroid1[0],centroid1[1], c=\"r\", s=100, marker=\"*\")\n",
    "plt.scatter(centroid2[0],centroid2[1], c=\"r\", s=100, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Compute the Euclidean distance between the data points and each one of the initialized centroids. Note that this step is the most computationally expensive part of the K-means algorithm because it involves (in the worst scenario) two loops with N iterations, where N is the number of samples in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.841141Z",
     "start_time": "2021-01-16T09:43:41.830611Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "X = df_clusters[['x','y']].values\n",
    "\n",
    "d_centroid1 = [euclidean_distance(sample, centroid1) for sample in X]\n",
    "d_centroid2 = [euclidean_distance(sample, centroid2) for sample in X]\n",
    "\n",
    "print(d_centroid1, d_centroid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Assign all the points to the closest cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:41.969492Z",
     "start_time": "2021-01-16T09:43:41.843256Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = {0: [], 1: []}\n",
    "\n",
    "for idx, pair in enumerate(zip(d_centroid1, d_centroid2)):\n",
    "    centroid_idx = np.argmin(pair)\n",
    "    clusters[centroid_idx].append(idx)\n",
    "\n",
    "print(\"indices of cluster 1 (blue): {}\".format(clusters[0]))\n",
    "print(\"indices of cluster 2 (red): {}\".format(clusters[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:42.058500Z",
     "start_time": "2021-01-16T09:43:41.972104Z"
    }
   },
   "outputs": [],
   "source": [
    "df_clusters.loc[clusters[0],'labels'] = 'blue'\n",
    "df_clusters.loc[clusters[1],'labels'] = 'red'\n",
    "df_clusters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:42.513245Z",
     "start_time": "2021-01-16T09:43:42.061098Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 3\", fontsize = 18)\n",
    "sns.scatterplot(data=df_clusters, x=\"x\", y=\"y\", hue=\"labels\")\n",
    "plt.scatter(centroid1[0],centroid1[1], c=\"k\", s=100, marker=\"*\")\n",
    "plt.scatter(centroid2[0],centroid2[1], c=\"k\", s=100, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T13:06:22.232154Z",
     "start_time": "2020-12-23T13:06:22.228222Z"
    }
   },
   "source": [
    "**STEP 4:** Update the position of the centroids by computing the mean value of the newly formed clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:42.922980Z",
     "start_time": "2021-01-16T09:43:42.521898Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "centroids_new = df_clusters.groupby([\"labels\"]).mean()[[\"x\",\"y\"]].values\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 4\", fontsize = 18)\n",
    "sns.scatterplot(data=df_clusters, x=\"x\", y=\"y\", hue=\"labels\")\n",
    "plt.scatter(centroids_new[:,0], centroids_new[:,1], c=\"k\", s=100, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:** Repeat step 2 to 4 until the positions of the centroids do not change within a given threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the K-Means algorithm is quite simple. However, there are still many interesting points to understand if we want to go deeper into the mathematical analysis of the method. For example, one may ask if the convergence is always guaranteed for any data set or if other distance metrics could be used rather than the Euclidean distance, or how much the converged cluster structure may depend on the centroid's initialization. Although these questions are relevant, they are beyond the scope of this introductory tutorial.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to use the K-means implementation of scikit-learn. To do that, we will still consider a 2D toy data set so we can visualize our results, but this time we can move on to a larger data set with more clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:43.317092Z",
     "start_time": "2021-01-16T09:43:42.925875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scikit-learn function to generate isotropic \n",
    "# Gaussian blobs for clustering\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:43.337145Z",
     "start_time": "2021-01-16T09:43:43.319269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make_blobs() returns a tuple of two values:\n",
    "# 1. A Numpy matrix with x- and y-values for each of the samples\n",
    "# 2. A 1D Numpy array containing the cluster labels for each sample\n",
    "data, true_labels = make_blobs(n_samples=[150,200,50], centers=None, \n",
    "                               cluster_std=2.8, random_state=135)\n",
    "\n",
    "df_clusters = pd.DataFrame(data, columns=['x', 'y'])\n",
    "df_clusters['true_labels'] = true_labels\n",
    "df_clusters.sort_values('true_labels', axis=0, ascending=True, inplace=True)\n",
    "df_clusters.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:43.976586Z",
     "start_time": "2021-01-16T09:43:43.339481Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_clusters, x='x', y='y', hue='true_labels', palette='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, this data set poses a much difficult scenario to K-Means since there is a significant overlap between the points belonging to clusters 0 (red) and 2 (green). Let's see how good the algorithm is at distinguishing these three clusters. \n",
    "\n",
    "First of all, we will apply the standardization procedure to our data using the StandardScaler class of scikit-learn as we did before, just to be sure that each feature is represented in the same scale. This is actually an important data preprocessing step for most distance-based machine learning algorithms because it can have a significant impact on the performance of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:43.995080Z",
     "start_time": "2021-01-16T09:43:43.979497Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "clusters_scaled = scaler.fit_transform(df_clusters[['x', 'y']])\n",
    "\n",
    "# Instantiate the KMeans class with the following arguments:\n",
    "kmeans = KMeans(n_clusters=3, n_init=30, max_iter=500, \n",
    "                init=\"random\", random_state=135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the parameter *n_cluster* that defines the number of clusters to be searched, the two most important parameters of KMeans class are:\n",
    "\n",
    "- *n_init* $\\longrightarrow$ sets the number of initializations to perform. This is important because two runs can converge on different cluster assignments. The default behavior for the scikit-learn algorithm is to perform ten k-means runs and return the results of the one with the lowest value of the cost function (sum of squared estimate of errors, SSE).\n",
    "\n",
    "- *max_iter* $\\longrightarrow$ sets the number of maximum iterations for each initialization of the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we apply the *fit* method in order to make the k-means algorithm search for the *best clusters structure*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:44.298962Z",
     "start_time": "2021-01-16T09:43:44.001689Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.fit(clusters_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics from the initialization run with the lowest SSE are available as attributes of kmeans after calling .fit():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:44.308776Z",
     "start_time": "2021-01-16T09:43:44.301042Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Lowest SSE value: {}\".format(kmeans.inertia_))\n",
    "print(\" \")\n",
    "print(\"Converged centroid positions:\\n\")\n",
    "print(kmeans.cluster_centers_)\n",
    "print(\" \")\n",
    "print(\"Number of iterations until convergence: {}\".format(kmeans.n_iter_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can obtain the cluster assignments as a one-dimensional NumPy array by using the kmeans.labels_ attribute as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:44.414657Z",
     "start_time": "2021-01-16T09:43:44.312230Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we could add this information (which is our predictions in this case) to the *df_clusters* data set. However, scikit-learn will arbitrarily assign numbers to each cluster without any particular order for the labels, so labels could have just as easily replaced all of the 1s with 0s and 0s with 1s and it would still be the same set of clusters. Since we know that the data set was previously organized in ascending order of the true labels, it seems that scikit-learn has arbitrarily inter-changed label 2 by 1. Therefore, to be able to compare the labels we need first to rename them to match the original order, and we can do this by using the *np.choose* command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:44.520830Z",
     "start_time": "2021-01-16T09:43:44.417499Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_labels = np.choose(kmeans.labels_,[0,2,1]).astype(np.int64)\n",
    "df_clusters['pred_labels'] = pred_labels\n",
    "df_clusters.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in this case, we have the true labels for the cluster assignments, we can compare this ground-truth reference with the labels predicted by k-means. One could even estimate precisely the performance or accuracy of the clustering model, and indeed we can see in the output above that k-means has made wrong cluster assignments for some points. For example, we can group our data set according to the true and predicted labels and then check how many right/wrong predictions the algorithm did. Let's try to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:44.614519Z",
     "start_time": "2021-01-16T09:43:44.523250Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_pred = df_clusters.groupby('pred_labels')['pred_labels'].count()\n",
    "count_true = df_clusters.groupby('true_labels')['true_labels'].count()\n",
    "df = pd.concat([count_true, count_pred], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, k-means was able to identify cluster 1 almost perfectly while the majority of the wrong label assignments given by the algorithm occurred for clusters 0 and 2. This happens because these two clusters are largely overlapping while data points related to cluster 1 appears relatively well separated from the other points (see the scatter plot shown above). However, it might be more interesting for some reason to split the data space comprised of clusters 0 and 2 into two equilibrated clusters with roughly the same number of data points. That is the nice thing about unsupervised learning because in this field we are usually interested in discovering patterns in the data that reveal statistically meaningful information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, one should keep in mind that in the unsupervised learning filed most of the data sets will not provide any label information related to possible classes or subgroups. This is essentially the case of the QM7 data set we have explored so far. Let's see how k-means performs in the QM7 data set with the exercise proposed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimal_k'></a>\n",
    "## Choosing the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T17:43:38.281270Z",
     "start_time": "2021-01-15T17:43:38.265197Z"
    }
   },
   "source": [
    "One important characteristic of k-means clustering is the requirement that the user must provide the number of clusters to be found in the data as an input parameter for the model. This is often considered as a downside of K-means because, in principle, the distribution of data points in the high-dimensional space is unknown. Although it is possible to gain some insights about the cluster's distribution by plotting the data projected in a 2D space using some dimensionality reduction technique, the loss of information can be significantly high such that the projected data may not represent the relationships (or distance) between data points in the original space. One possible way to mitigate this problem is by using some technique that allows seeking for the optimal number of clusters in a given dataset. The two most used approaches [to estimate the appropriate number of clusters](https://www.youtube.com/watch?v=lbR5br5yvrY) are\n",
    "\n",
    "- **Elbow method**\n",
    "- **Silhouette coefficient.**\n",
    "\n",
    "Here we will see an example of how to use the Elbow method to estimate the ideal number of clusters. The idea behind this method is pretty simple. It consists of running the k-means algorithm several times incrementing the number of cluster $k$ in each iteration. Then, we just plot the value of the loss function of k-means (i.e., the SSE value) as a function of $k$, and try to find the point at which the SSE curve starts to flatten out forming an \"elbow\". The Python code to run the Elbow method is quite simple as shown in the block cells below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:47.009755Z",
     "start_time": "2021-01-16T09:43:44.616931Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_params = {\"init\": \"random\", \"n_init\": 50, \n",
    "                 \"max_iter\": 500, \"random_state\": 42,}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_params)\n",
    "    kmeans.fit(clusters_scaled)\n",
    "    sse.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:43:47.329751Z",
     "start_time": "2021-01-16T09:43:47.012037Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot shown above, one can note that the SSE values always decrease when increasing k. As more centroids are added, the distance from each point to its closest centroid will decrease. Thus, the idea is to find a point in the plot that results in a reasonable trade-off between error and the number of clusters. In this example, the Elbow method gives k=2 as the ideal number of clusters. Based on the previous analysis is easy to understand this result. If you look again at the scatter plot of our data some cells above, you will see that there are two quite overlapping clusters, the red and green ones, and k-means is not able to identify these two clusters very well. Hence, for this particular dataset, the resulting number of clusters estimated via the Elbow method just reflects the intrinsic difficulties of k-means for clustering this type of data adequately. \n",
    "\n",
    "You can play a bit with the k-means method by changing the value of the \"random_state\" variable in the make_bloobs function to 42 to generate a different dataset. Then, you can run all the code cells again to see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex3'></a>\n",
    "## Exercise 3: Exploring K-Means with the QM7 dataset\n",
    "\n",
    "#### A. Use the K-means algorithm as implemented in scikit-learn to split the QM7 dataset into six distinct clusters. Remember that the Coulomb matrix representation used as input vector must be rescaled (use the StandardScaler()) before applying k-means. Print the sum of squared error (SSE) and the number of iterations required for this clustering task. Create a dataframe with one column for the total number of points assigned to each cluster and other two columns having the average and standard deviation of the number of atoms per cluster. Can you see any significant difference in the molecular systems between the obtained clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:44:48.597844Z",
     "start_time": "2021-01-04T15:44:16.090633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:44:48.616744Z",
     "start_time": "2021-01-04T15:44:48.603365Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:44:48.750810Z",
     "start_time": "2021-01-04T15:44:48.626327Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Select a few samples from the two biggest clusters and try to visualize the corresponding molecular structures. Can you see any significant difference in the molecules assigned to these different clusters? For each one of the three biggest clusters, make an histogram of the atomization energy divided by the number of atoms and show the results in the same plot for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:13:30.919483Z",
     "start_time": "2021-01-04T14:13:30.833637Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:13:30.996365Z",
     "start_time": "2021-01-04T14:13:30.922842Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T14:13:31.112838Z",
     "start_time": "2021-01-04T14:13:30.999327Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Try to estimate the ideal number of clusters for the rescaled QM7 dataset using the Elbow method. How many cluster did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. [Optional] Repeat all the steps of item A but now using the matrix resulting from a PCA decomposition as input to k-means. Apply the PCA dimensionality reduction and select only the first 100 principal components to create your new dataset. How different the distribution of molecules per cluster is when compared to the results of item A? Also, try to check the difference in the computing time.<br><br> Hint: You can use the [*groupby*](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) function of the pandas object to count the number of samples in each cluster, and also to compute the respective mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:50:46.375713Z",
     "start_time": "2021-01-04T15:50:45.168419Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:50:59.223023Z",
     "start_time": "2021-01-04T15:50:46.404720Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:50:59.260992Z",
     "start_time": "2021-01-04T15:50:59.230191Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T15:51:04.698466Z",
     "start_time": "2021-01-04T15:51:04.677371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1610802127708,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
